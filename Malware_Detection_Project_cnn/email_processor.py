# import os
# import base64
# import re
# import json
# import paramiko
# import requests
# from scp import SCPClient
# from googleapiclient.discovery import build
# from google_auth_oauthlib.flow import InstalledAppFlow
# from google.auth.transport.requests import Request
# from google.oauth2.credentials import Credentials
# from bs4 import BeautifulSoup
# from getpass import getpass

# # Configuration
# SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
# CREDENTIALS_PATH = 'credentials/client_secret.json'
# TOKEN_PATH = 'credentials/token.json'

# # Gmail authentication
# def get_gmail_service():
#    creds = None
#    if os.path.exists(TOKEN_PATH):
#       creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)
#    if not creds or not creds.valid:
#       if creds and creds.expired and creds.refresh_token:
#          creds.refresh(Request())
#       else:
#          flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)
#          creds = flow.run_local_server(port=0)
#       with open(TOKEN_PATH, 'w') as token:
#          token.write(creds.to_json())
#    return build('gmail', 'v1', credentials=creds)

# # Extract first Drive link from emails
# def extract_first_drive_link(service):
#    results = service.users().messages().list(userId='me', labelIds=['INBOX']).execute()
#    messages = results.get('messages', [])
#    for message in messages:
#       msg = service.users().messages().get(userId='me', id=message['id']).execute()
#       parts = msg['payload'].get('parts', [])
#       for part in parts:
#          body = part['body'].get('data', '')
#          decoded_body = base64.urlsafe_b64decode(body).decode('utf-8')
#          soup = BeautifulSoup(decoded_body, 'html.parser')
#          for link in soup.find_all('a', href=True):
#                href = link['href']
#                if "drive.google.com" in href:
#                   return href
#    raise Exception("No Google Drive link found.")

# # SSH and SCP functions using paramiko
# def create_ssh_client(server, port, user, password):
#    client = paramiko.SSHClient()
#    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
#    client.connect(server, port, user, password)
#    return client

# def transfer_link_and_execute_cnn(ssh, drive_link):
#    stdin, stdout, stderr = ssh.exec_command(
#       f'python3 /home/prashanna24/nndl-proj-wsl/kris.py "{drive_link}"'
#    )
#    print(stdout.read().decode())
#    print(stderr.read().decode())

# def fetch_result(ssh, remote_path, local_path):
#    with SCPClient(ssh.get_transport()) as scp:
#       scp.get(remote_path, local_path)

# # Main automation process
# def main():
#    gmail_service = get_gmail_service()

#    print("Extracting Google Drive image link from email...")
#    image_link = extract_first_drive_link(gmail_service)
#    print(f"Extracted Drive link: {image_link}")

#    # Ask for password only once
#    password = getpass("Enter WSL password: ")
#    ssh = create_ssh_client("192.168.242.152", 22, "prashanna24", password)

#    try:
#       # Run CNN directly with the link on WSL
#       print("Running CNN on WSL...")
#       transfer_link_and_execute_cnn(ssh, image_link)

#       # Fetch result
#       print("Fetching result from WSL...")
#       # fetch_result(ssh, "/home/prashanna24/nndl-proj-wsl/result.txt",
#       #             "C:\VS CODE/neural networks and deep learning/nndl-project2/results/result.txt")
#       fetch_result(ssh, "/home/prashanna24/nndl-proj-wsl/result.txt",
#                   r"C:\VS CODE\neural networks and deep learning\nndl-project2\results\result.txt")

#       print("Process completed successfully.")
#    finally:
#       ssh.close()

# if __name__ == '__main__':
#    main()


















# import os
# import base64
# import re
# import json
# import paramiko
# import requests
# from scp import SCPClient
# from googleapiclient.discovery import build
# from google_auth_oauthlib.flow import InstalledAppFlow
# from google.auth.transport.requests import Request
# from google.oauth2.credentials import Credentials
# from bs4 import BeautifulSoup
# from getpass import getpass
# import sys

# # Configuration
# SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
# CREDENTIALS_PATH = 'credentials/client_secret.json'
# TOKEN_PATH = 'credentials/token.json'

# # Gmail authentication
# def get_gmail_service():
#    creds = None
#    if os.path.exists(TOKEN_PATH):
#       creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)
#    if not creds or not creds.valid:
#       if creds and creds.expired and creds.refresh_token:
#          creds.refresh(Request())
#       else:
#          flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)
#          creds = flow.run_local_server(port=0)
#       with open(TOKEN_PATH, 'w') as token:
#          token.write(creds.to_json())
#    return build('gmail', 'v1', credentials=creds)

# # Extract first Drive link from emails
# def extract_first_drive_link(service):
#    results = service.users().messages().list(userId='me', labelIds=['INBOX'], maxResults=1).execute()
#    messages = results.get('messages', [])
   
#    if not messages:
#       raise Exception("No emails found in inbox")
   
#    message = messages[0]
#    msg = service.users().messages().get(userId='me', id=message['id']).execute()
   
#    # Search for links in both HTML and plain text parts
#    parts = [part for part in msg['payload']['parts']] if 'parts' in msg['payload'] else [msg['payload']]
   
#    for part in parts:
#       if 'data' in part['body']:
#          body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
#          soup = BeautifulSoup(body, 'html.parser')
#          links = soup.find_all('a', href=True)
#          for link in links:
#                if "drive.google.com" in link['href']:
#                   return link['href']
   
#    raise Exception("First email does not contain any Google Drive links")

# # SSH and SCP functions using paramiko
# def create_ssh_client(server, port, user, password):
#    client = paramiko.SSHClient()
#    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
#    client.connect(server, port, user, password)
#    return client

# def transfer_link_and_execute_cnn(ssh, drive_link):
#    stdin, stdout, stderr = ssh.exec_command(
#       f'python3 /home/prashanna24/nndl-proj-wsl/kris.py "{drive_link}"'
#    )
#    print(stdout.read().decode())
#    print(stderr.read().decode())

# def fetch_result(ssh, remote_path, local_path):
#    with SCPClient(ssh.get_transport()) as scp:
#       scp.get(remote_path, local_path)

# def main():
#    gmail_service = get_gmail_service()

#    print("Extracting Google Drive image link from first email...")
#    try:
#       image_link = extract_first_drive_link(gmail_service)
#       print(f"Extracted Drive link: {image_link}")
#    except Exception as e:
#       print(f"Error: {str(e)}")
#       print("Stopping execution.")
#       return  # Exit gracefully

#    # Ask for password only once
#    password = getpass("Enter WSL password: ")
#    ssh = create_ssh_client("192.168.242.152", 22, "prashanna24", password)

#    try:
#       # Run CNN directly with the link on WSL
#       print("Running CNN on WSL...")
#       transfer_link_and_execute_cnn(ssh, image_link)

#       # Fetch result
#       print("Fetching result from WSL...")
#       fetch_result(ssh, "/home/prashanna24/nndl-proj-wsl/result.txt",
#                   r"C:\VS CODE\neural networks and deep learning\nndl-project2\results\result.txt")

#       print("Process completed successfully.")
#    finally:
#       ssh.close()

# if __name__ == '__main__':
#    main()





















# import os
# import base64
# import re
# import json
# import paramiko
# import requests
# from scp import SCPClient
# from googleapiclient.discovery import build
# from google_auth_oauthlib.flow import InstalledAppFlow
# from google.auth.transport.requests import Request
# from google.oauth2.credentials import Credentials
# from bs4 import BeautifulSoup
# import sys

# sys.stdout.reconfigure(encoding='utf-8')
# sys.stderr.reconfigure(encoding='utf-8')
# # Configuration
# SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
# CREDENTIALS_PATH = 'credentials/client_secret.json'
# TOKEN_PATH = 'credentials/token.json'

# WSL_SERVER = "192.168.242.152"
# WSL_PORT = 22
# WSL_USER = "prashanna24"
# WSL_PASSWORD = "detroicitus"

# # Gmail authentication
# def get_gmail_service():
#    creds = None
#    if os.path.exists(TOKEN_PATH):
#       creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)
#    if not creds or not creds.valid:
#       if creds and creds.expired and creds.refresh_token:
#          creds.refresh(Request())
#       else:
#          flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)
#          creds = flow.run_local_server(port=0)
#       with open(TOKEN_PATH, 'w') as token:
#          token.write(creds.to_json())
#    return build('gmail', 'v1', credentials=creds)

# # Extract first Drive link from emails
# def extract_first_drive_link(service):
#    results = service.users().messages().list(userId='me', labelIds=['INBOX'], maxResults=1).execute()
#    messages = results.get('messages', [])
   
#    if not messages:
#       raise Exception("No emails found in inbox")
   
#    message = messages[0]
#    msg = service.users().messages().get(userId='me', id=message['id']).execute()
   
#    # Search for links in both HTML and plain text parts
#    parts = [part for part in msg['payload']['parts']] if 'parts' in msg['payload'] else [msg['payload']]
   
#    for part in parts:
#       if 'data' in part['body']:
#          body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
#          soup = BeautifulSoup(body, 'html.parser')
#          links = soup.find_all('a', href=True)
#          for link in links:
#                if "drive.google.com" in link['href']:
#                   return link['href']
   
#    raise Exception("First email does not contain any Google Drive links")

# # SSH and SCP functions using paramiko
# def create_ssh_client():
#    client = paramiko.SSHClient()
#    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
#    client.connect(WSL_SERVER, WSL_PORT, WSL_USER, WSL_PASSWORD)
#    return client

# def transfer_link_and_execute_cnn(ssh, drive_link):
#    stdin, stdout, stderr = ssh.exec_command(
#       f'python3 /home/prashanna24/nndl-proj-wsl/kris.py "{drive_link}"'
#    )
#    print(stdout.read().decode())
#    print(stderr.read().decode())

# def fetch_result(ssh, remote_path, local_path):
#    with SCPClient(ssh.get_transport()) as scp:
#       scp.get(remote_path, local_path)

# def main():
#    gmail_service = get_gmail_service()

#    print("Extracting Google Drive image link from first email...")
#    try:
#       image_link = extract_first_drive_link(gmail_service)
#       print(f"Extracted Drive link: {image_link}")
#    except Exception as e:
#       print(f"Error: {str(e)}")
#       print("Stopping execution.")
#       return  # Exit gracefully

#    ssh = create_ssh_client()

#    try:
#       # Run CNN directly with the link on WSL
#       print("Running CNN on WSL...")
#       transfer_link_and_execute_cnn(ssh, image_link)

#       # Fetch result
#       print("Fetching result from WSL...")
#       fetch_result(ssh, "/home/prashanna24/nndl-proj-wsl/result.txt",
#                   r"C:\VS CODE\neural networks and deep learning\nndl-project2\results\result.txt")

#       print("Process completed successfully.")
#    finally:
#       ssh.close()

# if __name__ == '__main__':
#    main()

























import os
import base64
import re
import json
import paramiko
import requests
from scp import SCPClient
from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from bs4 import BeautifulSoup
import sys
import email
from email.utils import parsedate_to_datetime

sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')
# Configuration
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']
CREDENTIALS_PATH = 'credentials/client_secret.json'
TOKEN_PATH = 'credentials/token.json'

WSL_SERVER = "192.168.242.152"
WSL_PORT = 22
WSL_USER = "prashanna24"
WSL_PASSWORD = "detroicitus"

# Gmail authentication
def get_gmail_service():
   creds = None
   if os.path.exists(TOKEN_PATH):
      creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)
   if not creds or not creds.valid:
      if creds and creds.expired and creds.refresh_token:
         creds.refresh(Request())
      else:
         flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)
         creds = flow.run_local_server(port=0)
      with open(TOKEN_PATH, 'w') as token:
         token.write(creds.to_json())
   return build('gmail', 'v1', credentials=creds)

# Extract email metadata and first Drive link from emails
def extract_email_info(service):
   results = service.users().messages().list(userId='me', labelIds=['INBOX'], maxResults=1).execute()
   messages = results.get('messages', [])
   
   if not messages:
      return {"error": "No emails found in inbox"}
   
   message = messages[0]
   msg = service.users().messages().get(userId='me', id=message['id']).execute()
   
   # Extract email metadata
   headers = msg['payload']['headers']
   metadata = {
      "from": next((h['value'] for h in headers if h['name'].lower() == 'from'), 'Unknown'),
      "to": next((h['value'] for h in headers if h['name'].lower() == 'to'), 'Unknown'),
      "subject": next((h['value'] for h in headers if h['name'].lower() == 'subject'), 'No Subject'),
      "date": next((h['value'] for h in headers if h['name'].lower() == 'date'), 'Unknown'),
   }
   
   # Search for links in both HTML and plain text parts
   parts = [part for part in msg['payload']['parts']] if 'parts' in msg['payload'] else [msg['payload']]
   
   drive_link = None
   for part in parts:
      if 'data' in part['body']:
         body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
         soup = BeautifulSoup(body, 'html.parser')
         links = soup.find_all('a', href=True)
         for link in links:
               if "drive.google.com" in link['href']:
                  drive_link = link['href']
                  break
         if drive_link:
            break
   
   metadata["drive_link"] = drive_link
   return metadata

# SSH and SCP functions using paramiko
def create_ssh_client():
   client = paramiko.SSHClient()
   client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
   client.connect(WSL_SERVER, WSL_PORT, WSL_USER, WSL_PASSWORD)
   return client

def transfer_link_and_execute_cnn(ssh, drive_link):
   stdin, stdout, stderr = ssh.exec_command(
      f'python3 /home/prashanna24/nndl-proj-wsl/kris.py "{drive_link}"'
   )
   print(stdout.read().decode())
   print(stderr.read().decode())

def fetch_result(ssh, remote_path, local_path):
   with SCPClient(ssh.get_transport()) as scp:
      scp.get(remote_path, local_path)

def main():
   gmail_service = get_gmail_service()

   print("Extracting email metadata and Google Drive image link from first email...")
   try:
      email_info = extract_email_info(gmail_service)
      
      # Save email metadata to a file for access from Flask
      with open(r"C:\VS CODE\neural networks and deep learning\nndl-project2\results\email_metadata.json", 'w') as f:
         json.dump(email_info, f)
      
      if "error" in email_info:
         print(f"Error: {email_info['error']}")
         return
      
      if not email_info.get("drive_link"):
         print("Error: No Google Drive links found in the first email")
         return
      
      print(f"Email from: {email_info['from']}")
      print(f"Subject: {email_info['subject']}")
      print(f"Extracted Drive link: {email_info['drive_link']}")
   except Exception as e:
      print(f"Error: {str(e)}")
      print("Stopping execution.")
      return  # Exit gracefully

   try:
      ssh = create_ssh_client()
      # Run CNN directly with the link on WSL
      print("Running CNN on WSL...")
      transfer_link_and_execute_cnn(ssh, email_info['drive_link'])

      # Fetch result
      print("Fetching result from WSL...")
      fetch_result(ssh, "/home/prashanna24/nndl-proj-wsl/result.txt",
                  r"C:\VS CODE\neural networks and deep learning\nndl-project2\results\result.txt")

      print("Process completed successfully.")
      ssh.close()
   except Exception as e:
      print(f"Error during WSL processing: {str(e)}")
      return

if __name__ == '__main__':
   main()